# Polaris AI v2

## ğŸš€ Sobre o Projeto

Polaris Ã© um assistente inteligente desenvolvido utilizando *FastAPI*, *Llama*, *LangChain* e *MongoDB* para oferecer inferÃªncias contextuais aprimoradas. Ele possui memÃ³ria de conversaÃ§Ã£o e armazenamento de memÃ³rias de usuÃ¡rios, permitindo respostas mais precisas e personalizadas ao longo do tempo.

## âœ¨ Recursos Principais
- ğŸŒ **API REST**: Desenvolvida em *FastAPI* para alta performance.
- ğŸ§  **Modelo LLaMA**: Utiliza *Meta-Llama-3-8B-Instruct* para inferÃªncias inteligentes.
- ğŸ—„ï¸ **MemÃ³ria Persistente**: Armazena informaÃ§Ãµes importantes no *MongoDB*.
- ğŸ”„ **MemÃ³ria TemporÃ¡ria**: Utiliza *LangChain Memory* para manter contexto recente.
- ğŸ”¥ **InferÃªncia Eficiente**: Implementa otimizaÃ§Ãµes para respostas rÃ¡pidas e relevantes.
- ğŸ”§ **Logging Estruturado**: Sistema de logs para rastreamento detalhado.
- ğŸ”“ **CORS Middleware**: Suporte a conexÃµes de diversas origens.

## ğŸ—ï¸ Tecnologias Utilizadas

- **[FastAPI](https://fastapi.tiangolo.com/)** - Framework para desenvolvimento rÃ¡pido de APIs
- **[Llama (llama-cpp)](https://github.com/ggerganov/llama.cpp)** - ImplementaÃ§Ã£o eficiente de modelos LLaMA
- **[LangChain](https://python.langchain.com/)** - Gerenciamento de memÃ³ria e histÃ³rico de conversaÃ§Ã£o
- **[MongoDB](https://www.mongodb.com/)** - Banco de dados NoSQL para persistÃªncia de informaÃ§Ãµes
- **[ChromaDB](https://www.trychroma.com/)** - Vetorizador de memÃ³rias contextuais
- **[Uvicorn](https://www.uvicorn.org/)** - Servidor ASGI para execuÃ§Ã£o do FastAPI

## ğŸ“¦ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### ğŸ“Œ Requisitos
- Python 3.9+
- MongoDB rodando na porta padrÃ£o (ou configurado em `MONGO_URI`)
- Modelo LLaMA baixado para `./models/`
- DependÃªncias Python instaladas

### âš™ï¸ Passos para ConfiguraÃ§Ã£o
1. Clone o repositÃ³rio:
   ```sh
   git clone https://github.com/Tech-Tweakers/polaris-python-api.git
   cd polaris
   ```

2. Crie e ative um ambiente virtual (opcional, mas recomendado):
   ```sh
   python -m venv venv
   source venv/bin/activate  # Linux/macOS
   venv\Scripts\activate  # Windows
   ```

3. Instale as dependÃªncias:
   ```sh
   pip install -r requirements.txt
   ```

4. Execute a Docker Composer:
   ```sh
   docker-compose up --build -d
   ```

5. Execute a API:
   ```sh
   python polaris_api.py
   ```

## ğŸ”¥ Endpoints DisponÃ­veis

### ğŸ”¹ InferÃªncia com o Modelo
**POST** `/inference/`
- **DescriÃ§Ã£o**: Envia um prompt para o modelo e recebe uma resposta inteligente.
- **Body (JSON)**:
  ```json
  {
    "prompt": "Qual Ã© a capital da FranÃ§a?",
    "session_id": "usuario123"
  }
  ```
- **Resposta (JSON)**:
  ```json
  {
    "resposta": "A capital da FranÃ§a Ã© Paris."
  }
  ```

## ğŸ› ï¸ Estrutura do Projeto
```
polaris/
â”‚â”€â”€ models/                  # Modelos LLaMA
â”‚â”€â”€ chroma_db/               # Base de dados vetorizada do ChromaDB
â”‚â”€â”€ logs/                    # Arquivos de log
â”‚â”€â”€ main.py                  # CÃ³digo principal
â”‚â”€â”€ requirements.txt         # DependÃªncias
â”‚â”€â”€ polaris_prompt.txt       # Prompt de configuraÃ§Ã£o do modelo
â”‚â”€â”€ keywords.txt             # Palavras-chave para memÃ³rias importantes
```

## ğŸ“Œ ConsideraÃ§Ãµes Finais

Polaris foi projetado para ser um assistente poderoso e altamente escalÃ¡vel. Com suporte a memÃ³rias persistentes e contexto dinÃ¢mico, ele se torna um sistema de IA altamente interativo e personalizÃ¡vel.

ğŸ’¡ Se desejar contribuir com melhorias, sinta-se Ã  vontade para abrir um *Pull Request*! ğŸš€
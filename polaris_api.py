import asyncio
import os
import time
import traceback
import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
from llama_cpp import Llama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.memory import ConversationBufferMemory
from pymongo import MongoClient
import uvicorn

# ğŸ”¹ ConfiguraÃ§Ã£o do log
LOG_FILE = "polaris.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def log_info(message: str): logging.info(f"ğŸ”¹ {message}")
def log_success(message: str): logging.info(f"âœ… {message}")
def log_warning(message: str): logging.warning(f"âš ï¸ {message}")
def log_error(message: str): logging.error(f"âŒ {message}")

# ğŸ”¹ ConfiguraÃ§Ã£o do modelo
MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q4_0.gguf"
NUM_CORES = 6
MODEL_CONTEXT_SIZE = 4096
MODEL_BATCH_SIZE = 8

# ğŸ”¹ ConexÃ£o com o MongoDB
MONGO_URI = "mongodb://admin:admin123@localhost:27017/polaris_db?authSource=admin"
client = MongoClient(MONGO_URI)
db = client["polaris_db"]
collection = db["user_memory"]

# ğŸ”¹ Inicializa FastAPI
app = FastAPI()

# ğŸ”¹ Inicializa LangChain Memory
log_info("ğŸ”¹ Configurando memÃ³ria do LangChain...")

embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedder)
memory = ConversationBufferMemory(memory_key="history", output_key="output", return_messages=True)

# ğŸ”¹ Classe para Llama
class LlamaRunnable:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.llm = None

    def load(self):
        try:
            if self.llm is None:
                log_info("ğŸ”¹ Carregando modelo LLaMA...")
                self.llm = Llama(model_path=self.model_path, n_threads=NUM_CORES, n_ctx=MODEL_CONTEXT_SIZE, batch_size=MODEL_BATCH_SIZE, verbose=False)
                log_success("âœ… Modelo LLaMA carregado com sucesso!")
        except Exception as e:
            log_error(f"âŒ Erro ao carregar o modelo LLaMA: {str(e)}")
            raise e

    def close(self):
        if self.llm is not None:
            log_info("ğŸ”¹ Fechando o modelo LLaMA...")
            del self.llm
            self.llm = None
            log_success("âœ… Modelo LLaMA fechado com sucesso!")

    def invoke(self, prompt: str):
        if self.llm is None:
            log_error("âŒ Erro: Modelo nÃ£o carregado!")
            raise HTTPException(status_code=500, detail="Modelo nÃ£o carregado!")

        log_info(f"ğŸ“œ Enviando prompt ao modelo:\n{prompt}")

        start_time = time.time()
        response = self.llm(prompt, stop=["\n"], max_tokens=100, echo=False)
        end_time = time.time()

        elapsed_time = end_time - start_time
        log_info(f"âš¡ Tempo de inferÃªncia: {elapsed_time:.3f} segundos")

        if "choices" in response and response["choices"]:
            resposta = response["choices"][0]["text"].strip()
            log_success(f"âœ… Resposta gerada pelo modelo: {resposta}")
            return resposta

        log_error("âŒ Erro: Resposta do modelo vazia ou invÃ¡lida!")
        return "Erro ao gerar resposta."

llm = LlamaRunnable(model_path=MODEL_PATH)

@app.on_event("startup")
async def startup_event():
    llm.load()

@app.on_event("shutdown")
async def shutdown_event():
    llm.close()

class InferenceRequest(BaseModel):
    prompt: str
    session_id: Optional[str] = "default_session"

# ğŸ”¹ Recuperar memÃ³rias do MongoDB
def get_memories():
    memories = collection.find().sort("timestamp", -1).limit(5)
    texts = [mem["text"] for mem in memories]
    log_info(f"ğŸ“Œ Recuperadas {len(texts)} memÃ³rias do MongoDB.")
    return texts

# ğŸ”¹ Recuperar contexto do ChromaDB
def get_recent_memories():
    """Recupera as Ãºltimas mensagens armazenadas no LangChain para fornecer contexto recente."""
    history = memory.load_memory_variables({})["history"]

    if not isinstance(history, list):
        return []

    recent_memories = "\n".join(
        [f"UsuÃ¡rio: {msg.content}" if isinstance(msg, HumanMessage) else f"Polaris: {msg.content}" for msg in history]
    )

    log_info(f"ğŸ“Œ Recuperadas {len(history)} mensagens da memÃ³ria temporÃ¡ria do LangChain.")
    return recent_memories


def save_to_langchain_memory(user_input, response):
    """Salva a conversa no cache temporÃ¡rio do LangChain, mantendo um histÃ³rico curto."""
    try:
        # ğŸ”¹ Adiciona a conversa no cache temporÃ¡rio
        memory.save_context({"input": user_input}, {"output": response})  # Garante que 'output' sempre existe

        # ğŸ”¹ Recupera o histÃ³rico atualizado
        history = memory.load_memory_variables({})["history"]

        # ğŸ”¹ Se o histÃ³rico ultrapassar 10 mensagens, removemos as mais antigas
        if len(history) > 10:
            log_warning("âš ï¸ MemÃ³ria temporÃ¡ria cheia, removendo mensagens mais antigas...")

            # Limpa a memÃ³ria e reinsere apenas as Ãºltimas 10 mensagens
            memory.clear()
            for i in range(len(history) - 10, len(history)):  # MantÃ©m as 10 mais recentes
                entry = history[i]
                if isinstance(entry, HumanMessage):
                    memory.save_context({"input": entry.content}, {"output": ""})  # Salva sem erro
                elif isinstance(entry, AIMessage):
                    memory.save_context({"input": "", "output": entry.content})  # Salva sem erro

        log_success("âœ… MemÃ³ria temporÃ¡ria do LangChain atualizada com sucesso!")

    except Exception as e:
        log_error(f"âŒ Erro ao salvar na memÃ³ria temporÃ¡ria do LangChain: {str(e)}")

def save_to_chroma_limited(user_input):
    """Salva no ChromaDB mantendo no mÃ¡ximo 10 entradas recentes"""
    try:
        # Recupera todas as memÃ³rias salvas no ChromaDB
        all_docs = vectorstore.similarity_search("", k=100)  # Busca todas as entradas
        total_memories = len(all_docs)

        # Se jÃ¡ temos 10 ou mais memÃ³rias, removemos as mais antigas
        if total_memories >= 10:
            log_warning(f"âš ï¸ Limite de 10 entradas atingido. Removendo as mais antigas...")
            for i in range(total_memories - 9):  # Remove apenas as mais antigas para manter 10 no total
                vectorstore.delete([all_docs[i].id])

        # Adiciona a nova entrada
        vectorstore.add_texts([user_input])
        log_success(f"âœ… InformaÃ§Ã£o armazenada no ChromaDB: {user_input}")

    except Exception as e:
        log_error(f"âŒ Erro ao salvar no ChromaDB: {str(e)}")

# ğŸ”¹ Armazenar informaÃ§Ãµes no MongoDB
def save_to_mongo(user_input):
    """Salva informaÃ§Ãµes no MongoDB e tambÃ©m armazena no ChromaDB com limite de 10 entradas"""
    try:
        existing_entry = collection.find_one({"text": user_input})
        if existing_entry:
            log_warning(f"âš ï¸ Entrada duplicada detectada, nÃ£o serÃ¡ salva: {user_input}")
            return

        doc = {"text": user_input, "timestamp": datetime.utcnow()}
        result = collection.insert_one(doc)
        if result.inserted_id:
            log_success(f"âœ… InformaÃ§Ã£o armazenada no MongoDB: {user_input}")

            # ğŸ”¹ Agora salvamos no ChromaDB com limite
            save_to_chroma_limited(user_input)
    except Exception as e:
        log_error(f"âŒ Erro ao salvar no MongoDB: {str(e)}")

# ğŸ”¹ Carrega o prompt de instruÃ§Ã£o do arquivo
def load_prompt_from_file(file_path="polaris_prompt.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        log_warning(f"âš ï¸ Arquivo {file_path} nÃ£o encontrado! Usando um prompt padrÃ£o.")
        return """\
        ### InstruÃ§Ãµes:
        VocÃª Ã© Polaris, um assistente inteligente.
        Responda de forma clara e objetiva, utilizando informaÃ§Ãµes do histÃ³rico e memÃ³rias disponÃ­veis.
        Se nÃ£o souber a resposta, seja honesto e nÃ£o invente informaÃ§Ãµes.

        Agora, aqui estÃ¡ a conversa atual:
        """

def load_keywords_from_file(file_path="keywords.txt"):
    """Carrega a lista de palavras-chave do arquivo especificado."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            keywords = [line.strip().lower() for line in file.readlines() if line.strip()]
            log_info(f"ğŸ“‚ Palavras-chave carregadas do arquivo ({len(keywords)} palavras).")
            return keywords
    except FileNotFoundError:
        log_warning(f"âš ï¸ Arquivo {file_path} nÃ£o encontrado! Usando palavras-chave padrÃ£o.")
        return ["meu nome Ã©", "eu moro em", "eu gosto de"]

def trim_langchain_memory():
    """MantÃ©m apenas as Ãºltimas 10 mensagens no cache temporÃ¡rio do LangChain sem quebrar o formato."""
    try:
        history = memory.load_memory_variables({})["history"]

        if not isinstance(history, list):
            return

        # ğŸ”¹ Se o histÃ³rico tiver mais de 10 mensagens, reduzimos para as 10 mais recentes
        if len(history) > 10:
            log_warning("âš ï¸ âš ï¸ MemÃ³ria temporÃ¡ria cheia, removendo mensagens mais antigas...")
            memory.chat_memory.messages = history[-10:]  # MantÃ©m apenas as 10 mais recentes

        log_success("âœ… MemÃ³ria temporÃ¡ria ajustada sem perda de formato!")

    except Exception as e:
        log_error(f"âŒ âŒ Erro ao ajustar memÃ³ria temporÃ¡ria do LangChain: {str(e)}")

from langchain.schema import HumanMessage, AIMessage

@app.post("/inference/")
async def inference(request: InferenceRequest):
    session_id = request.session_id or "default_session"
    log_info(f"ğŸ“¥ Nova solicitaÃ§Ã£o de inferÃªncia: {request.prompt}")

    keywords = load_keywords_from_file()

    # ğŸ”¹ Salva no MongoDB se for informaÃ§Ã£o relevante
    if any(kw in request.prompt.lower() for kw in keywords):
        save_to_mongo(request.prompt)

    # ğŸ”¹ Ajusta a memÃ³ria temporÃ¡ria antes de salvar novas entradas
    trim_langchain_memory()

    # ğŸ”¹ Recupera memÃ³rias
    mongo_memories = get_memories()
    recent_memories = get_recent_memories()

    # ğŸ”¹ ConstrÃ³i contexto
    context_pieces = []
    if mongo_memories:
        context_pieces.append("ğŸ“Œ MemÃ³ria do UsuÃ¡rio:\n" + "\n".join(mongo_memories))
    if recent_memories:
        context_pieces.append("ğŸ“Œ Conversa recente:\n" + recent_memories)

    context = "\n\n".join(context_pieces)

    # ğŸ”¹ Carrega o prompt de instruÃ§Ã£o
    prompt_instrucoes = load_prompt_from_file()

    # ğŸ”¹ Monta prompt final
    full_prompt = f"""{prompt_instrucoes}

--- CONTEXTO ---
{context}

--- CONVERSA ATUAL ---
UsuÃ¡rio: {request.prompt}

Polaris:"""

    # ğŸ”¹ Gera resposta
    resposta = llm.invoke(full_prompt)

    # ğŸ”¹ Salva nova interaÃ§Ã£o na memÃ³ria temporÃ¡ria
    memory.save_context({"input": request.prompt}, {"output": resposta})

    return {"resposta": resposta}

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

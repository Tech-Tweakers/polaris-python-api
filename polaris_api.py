import asyncio
import os
import time
import traceback
import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List
from llama_cpp import Llama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_core.runnables import Runnable
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ConversationBufferMemory
import uvicorn

# üîπ Configura√ß√£o do log
class LogColors:
    INFO = "\033[94m"
    SUCCESS = "\033[92m"
    WARNING = "\033[93m"
    ERROR = "\033[91m"
    RESET = "\033[0m"

def log_info(message: str): logging.info(f"{LogColors.INFO}üîπ {message}{LogColors.RESET}")
def log_success(message: str): logging.info(f"{LogColors.SUCCESS}‚úÖ {message}{LogColors.RESET}")
def log_warning(message: str): logging.warning(f"{LogColors.WARNING}‚ö†Ô∏è {message}{LogColors.RESET}")
def log_error(message: str): logging.error(f"{LogColors.ERROR}‚ùå {message}{LogColors.RESET}")

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# üîπ Configura√ß√£o do modelo
MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q4_0.gguf"

NUM_CORES = 6
MODEL_CONTEXT_SIZE = 2048
MODEL_BATCH_SIZE = 8

# üîπ Inicializa FastAPI
app = FastAPI()

# üîπ Inicializa LangChain Memory
log_info("üîπ Configurando mem√≥ria do LangChain...")

embedder = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedder)

# üîπ Mem√≥ria de conversa√ß√£o curta
memory = ConversationBufferMemory(memory_key="history", return_messages=True)

# üîπ Classe para Llama (Agora um `Runnable`)
class LlamaRunnable(Runnable):
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.llm = None

    def load(self):
        if self.llm is None:
            log_info("üîπ Carregando modelo LLaMA...")
            self.llm = Llama(model_path=self.model_path, n_threads=NUM_CORES, n_ctx=4096, batch_size=MODEL_BATCH_SIZE)
            log_success("‚úÖ Modelo LLaMA carregado com sucesso!")

    def close(self):
        """ Fecha corretamente o modelo LLaMA para evitar erro de finaliza√ß√£o. """
        if self.llm is not None:
            log_info("üîπ Fechando o modelo LLaMA...")
            del self.llm  # Remove o objeto para liberar a mem√≥ria
            self.llm = None
            log_success("‚úÖ Modelo LLaMA fechado com sucesso!")

    def invoke(self, messages):
        if self.llm is None:
            log_error("‚ùå Erro: Modelo n√£o carregado!")
            raise HTTPException(status_code=500, detail="Modelo n√£o carregado!")

        # üîπ Corrigindo extra√ß√£o da √∫ltima mensagem e garantindo que `user_prompt` sempre existe
        user_prompt = "Desculpe, n√£o encontrei uma pergunta v√°lida."  # Default

        if isinstance(messages, list) and messages:  # Garante que `messages` √© uma lista e n√£o est√° vazia
            full_prompt = "\n".join(
                [msg["content"] if isinstance(msg, dict) else msg.content for msg in messages]
            )  # <- Corre√ß√£o aqui
            user_prompt = messages[-1]["content"] if isinstance(messages[-1], dict) else messages[-1].content  # <- Corre√ß√£o aqui
        else:
            full_prompt = user_prompt

        # üîπ Adiciona um prefixo ao prompt
        full_prompt = f"Usu√°rio: {full_prompt}\nPolaris:"

        # üîπ Mede tempo de infer√™ncia
        start_time = time.time()
        response = self.llm(full_prompt, stop=["\n"], max_tokens=100, echo=False)
        end_time = time.time()

        # üîπ Corre√ß√£o do c√°lculo do tempo de infer√™ncia
        elapsed_time = end_time - start_time  # Tempo decorrido em segundos
        hours = int(elapsed_time // 3600)
        minutes = int((elapsed_time % 3600) // 60)
        seconds = elapsed_time % 60

        formatted_time = f"{hours:02}:{minutes:02}:{seconds:06.3f}"
        log_info(f"‚ö° Tempo de infer√™ncia: {formatted_time}")

        # üîπ Verifica se a resposta do modelo existe antes de acess√°-la
        if "choices" in response and response["choices"]:
            return response["choices"][0]["text"].strip()

        log_error("‚ùå Erro: Resposta do modelo vazia ou inv√°lida!")
        return "Erro ao gerar resposta."

llm = LlamaRunnable(model_path=MODEL_PATH)

# üîπ Carrega o modelo antes do primeiro uso
@app.on_event("startup")
async def startup_event():
    llm.load()

@app.on_event("shutdown")
async def shutdown_event():
    llm.close()

# üîπ Modelo de infer√™ncia
class InferenceRequest(BaseModel):
    prompt: str
    session_id: Optional[str] = "default_session"

# üîπ Fun√ß√£o para interagir com a IA
@app.post("/inference/")
async def inference(request: InferenceRequest):
    session_id = request.session_id or "default_session"

    # üîπ Obt√©m o hist√≥rico salvo na mem√≥ria curta
    history = memory.load_memory_variables({})["history"]
    
    # üîπ Certifica-se de que `history` √© uma lista antes de modificar
    if not isinstance(history, list):
        history = []
    
    history.append({"role": "user", "content": request.prompt})

    # üîç **Passo 2: Recuperar mem√≥ria de longo prazo da ChromaDB**
    retrieved_docs = vectorstore.similarity_search(request.prompt, k=1)
    if retrieved_docs:
        log_info(f"üîç Mem√≥ria recuperada: {retrieved_docs[0].page_content}")
        # Adiciona a mem√≥ria recuperada ao hist√≥rico para refor√ßar a resposta
        history.append({"role": "system", "content": f"Lembre-se: {retrieved_docs[0].page_content}"})

    # üîπ Faz a infer√™ncia com base no hist√≥rico atualizado
    resposta = llm.invoke(history)

    # üîπ Salva resposta na mem√≥ria curta
    memory.save_context({"input": request.prompt}, {"output": resposta})

    # üîπ Categorias expandidas de palavras-chave para detectar informa√ß√µes importantes
    KEYWORDS = [
        # üîπ Identidade
        "meu nome √©", "sou conhecido como", "me chamam de", "meu apelido √©",
        "eu tenho", "eu sou um", "eu sou uma", "minha idade √©", "tenho anos",
        "trabalho como", "minha profiss√£o √©", "sou formado em", "me formei em",
        "minha religi√£o √©", "sou religioso", "sou ateu", "sou agn√≥stico", "acredito em",
        "sou brasileiro", "minha nacionalidade √©", "falo", "aprendi a falar",
        "minha cren√ßa √©", "eu sigo", "minha filosofia de vida √©",

        # üîπ Localiza√ß√£o
        "eu moro em", "eu vivo em", "sou de", "nasci em", "minha cidade √©", "meu bairro √©",
        "moro no estado de", "meu endere√ßo √©", "eu frequento", "costumo ir em",
        "trabalho em", "estudo em", "minha escola √©", "minha faculdade √©", "minha universidade √©",

        # üîπ Prefer√™ncias
        "eu gosto de", "adoro", "sou f√£ de", "meu hobby √©", "prefiro", 
        "minha comida favorita √©", "meu prato favorito √©", "minha bebida favorita √©",
        "minha banda favorita √©", "meu filme favorito √©", "meu livro favorito √©",
        "meu autor favorito √©", "minha s√©rie favorita √©", "meu esporte favorito √©",
        "eu tor√ßo para", "meu time √©", "minha cor favorita √©", "eu amo", "eu odeio",
        "sou vegetariano", "sou vegano", "n√£o como", "sou al√©rgico a", "eu n√£o gosto de",
        "meu animal favorito √©", "tenho um pet", "meu cachorro se chama", "meu gato se chama",
        
        # üîπ Relacionamentos
        "tenho um amigo chamado", "minha m√£e se chama", "meu pai se chama",
        "meu filho se chama", "minha filha se chama", "minha esposa se chama",
        "meu marido se chama", "meu namorado se chama", "minha namorada se chama",
        "estou solteiro", "estou casado", "estou noivo", "estou divorciado",
        "tenho irm√£os", "meu irm√£o se chama", "minha irm√£ se chama",
        
        # üîπ Eventos e experi√™ncias
        "viajei para", "j√° estive em", "fui para", "participei de", 
        "me formei em", "trabalhei em", "estudei em", "morei em", "j√° morei em",
        "meu primeiro emprego foi", "fiz um interc√¢mbio", "j√° dei aula de",
        "j√° apresentei", "j√° ganhei um pr√™mio", "j√° fui volunt√°rio em",
        
        # üîπ Rotina e h√°bitos
        "acordo √†s", "durmo √†s", "trabalho das", "estudo das", "tenho aula de",
        "costumo acordar", "tenho o h√°bito de", "minha rotina √©", "todos os dias eu",
        "eu sempre fa√ßo", "normalmente eu", "costumo fazer", "nos finais de semana eu",
        "tenho o costume de", "minha manh√£ √©", "minha noite √©",
        
        # üîπ Sa√∫de e bem-estar
        "fa√ßo academia", "pratico esportes", "corro", "ando de bicicleta",
        "tenho press√£o alta", "tenho diabetes", "sou intolerante a", "sou al√©rgico a",
        "estou em tratamento para", "fa√ßo dieta", "n√£o como", "sou fitness",
        "meu m√©dico recomendou", "eu medito", "eu fa√ßo yoga", "eu bebo muita √°gua",
        
        # üîπ Objetivos e sonhos
        "meu sonho √©", "quero aprender a", "quero viajar para", "meu objetivo √©",
        "planejo me mudar para", "quero me formar em", "quero trabalhar com",
        "quero morar em", "pretendo abrir um neg√≥cio", "meu desejo √©", "gostaria de",
        
        # üîπ Personalidade e comportamentos
        "tenho medo de", "sou ansioso", "me considero uma pessoa", "eu sou t√≠mido",
        "eu sou extrovertido", "sou uma pessoa organizada", "sou bagunceiro",
        "eu costumo procrastinar", "eu sou perfeccionista", "eu sou impulsivo",
        "eu gosto de desafios", "eu evito conflitos", "sou muito emocional",
        "tenho dificuldade em", "sou bom em", "sou especialista em",
        
        # üîπ Tecnologia e uso de internet
        "eu uso redes sociais", "tenho conta no Instagram", "uso Twitter",
        "trabalho com tecnologia", "sou programador", "sou desenvolvedor",
        "uso Linux", "uso Windows", "uso Mac", "meu celular √© um", "meu notebook √© um",
        "meu jogo favorito √©", "jogo videogame", "tenho um PlayStation", "tenho um Xbox",
        "gosto de assistir lives", "sigo canais no YouTube",
    ]

    # üîπ Verifica se o prompt cont√©m informa√ß√µes importantes para armazenar
    if any(kw in request.prompt.lower() for kw in KEYWORDS):
        vectorstore.add_texts([request.prompt])
        log_info(f"üìå Informa√ß√£o armazenada na mem√≥ria de longo prazo: {request.prompt}")

    return {"resposta": resposta}

# üîπ Aplicando CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# üîπ Rodar servidor
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

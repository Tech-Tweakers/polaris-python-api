import time
import logging
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from pydantic import BaseModel
from typing import Optional
from llama_cpp import Llama
from langchain_chroma import Chroma
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_huggingface import HuggingFaceEmbeddings
from pymongo import MongoClient
import uvicorn

LOG_FILE = "polaris.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def log_info(message: str): logging.info(f"üîπ {message}")
def log_success(message: str): logging.info(f"‚úÖ {message}")
def log_warning(message: str): logging.warning(f"‚ö†Ô∏è {message}")
def log_error(message: str): logging.error(f"‚ùå {message}")

MODEL_PATH = "./models/Meta-Llama-3-8B-Instruct.Q4_0.gguf"
NUM_CORES = 6
MODEL_CONTEXT_SIZE = 4096
MODEL_BATCH_SIZE = 8

MONGODB_HISTORY = 10
LANGCHAIN_HISTORY = 6

MONGO_URI = "mongodb://admin:admin123@localhost:27017/polaris_db?authSource=admin"
client = MongoClient(MONGO_URI)
db = client["polaris_db"]
collection = db["user_memory"]

app = FastAPI()

log_info("Configurando mem√≥ria do LangChain...")

embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedder)

history = ChatMessageHistory()
memory = ConversationBufferMemory(chat_memory=history, return_messages=True)  # ‚úÖ memory_key REMOVIDO


class LlamaRunnable:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.llm = None

    def load(self):
        try:
            if self.llm is None:
                log_info("Carregando modelo LLaMA...")
                self.llm = Llama(
                    model_path=self.model_path, 
                    n_threads=NUM_CORES, 
                    n_ctx=MODEL_CONTEXT_SIZE, 
                    batch_size=MODEL_BATCH_SIZE, 
                    verbose=False,
                    use_mlock=True
                )
                log_success("Modelo LLaMA carregado com sucesso!")
        except Exception as e:
            log_error(f"Erro ao carregar o modelo LLaMA: {str(e)}")
            raise e

    def close(self):
        if self.llm is not None:
            log_info("Fechando o modelo LLaMA...")
            del self.llm
            self.llm = None
            log_success("Modelo LLaMA fechado com sucesso!")

    def invoke(self, prompt: str):
        if self.llm is None:
            log_error("Erro: Modelo n√£o carregado!")
            raise HTTPException(status_code=500, detail="Modelo n√£o carregado!")

        log_info(f"üìú Enviando prompt ao modelo:\n{prompt}")

        start_time = time.time()
        response = self.llm(prompt, stop=["\n", "---"], max_tokens=1024, echo=False)
        end_time = time.time()

        elapsed_time = end_time - start_time
        log_info(f"‚ö° Tempo de infer√™ncia: {elapsed_time:.3f} segundos")

        if "choices" in response and response["choices"]:
            resposta = response["choices"][0]["text"].strip()
            log_success(f"Resposta gerada pelo modelo: {resposta}")
            return resposta

        log_error("Erro: Resposta do modelo vazia ou inv√°lida!")
        return "Erro ao gerar resposta."

llm = LlamaRunnable(model_path=MODEL_PATH)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Evento de startup
    llm.load()
    yield  # Permite que a aplica√ß√£o rode enquanto a inicializa√ß√£o ocorre
    # Evento de shutdown
    llm.close()

app = FastAPI(lifespan=lifespan)

class InferenceRequest(BaseModel):
    prompt: str
    session_id: Optional[str] = "default_session"

def get_memories():
    memories = collection.find().sort("timestamp", -1).limit(MONGODB_HISTORY)
    texts = [mem["text"] for mem in memories]
    log_info(f"üìå Recuperadas {len(texts)} mem√≥rias do MongoDB.")
    return texts

def get_recent_memories():
    history = memory.load_memory_variables({})["history"]

    if not isinstance(history, list):
        return []

    recent_memories = "\n".join(
        [f"Usu√°rio: {msg.content}" if isinstance(msg, HumanMessage) else f"Polaris: {msg.content}" for msg in history]
    )

    log_info(f"üìå Recuperadas {len(history)} mensagens da mem√≥ria tempor√°ria do LangChain.")
    return recent_memories


def save_to_langchain_memory(user_input, response):
    try:
        memory.save_context({"input": user_input}, {"output": response})
        history = memory.load_memory_variables({})["history"]

        if len(history) > LANGCHAIN_HISTORY:
            log_warning("Mem√≥ria tempor√°ria cheia, removendo mensagens mais antigas...")
            memory.clear()
            for i in range(len(history) - LANGCHAIN_HISTORY, len(history)):
                entry = history[i]
                if isinstance(entry, HumanMessage):
                    memory.save_context({"input": entry.content}, {"output": ""})
                elif isinstance(entry, AIMessage):
                    memory.save_context({"input": "", "output": entry.content})

        log_success("Mem√≥ria tempor√°ria do LangChain atualizada com sucesso!")

    except Exception as e:
        log_error(f"Erro ao salvar na mem√≥ria tempor√°ria do LangChain: {str(e)}")

def save_to_mongo(user_input):
    try:
        existing_entry = collection.find_one({"text": user_input})
        if existing_entry:
            log_warning(f"Entrada duplicada detectada, n√£o ser√° salva: {user_input}")
            return

        doc = {"text": user_input, "timestamp": datetime.utcnow()}
        result = collection.insert_one(doc)
        if result.inserted_id:
            log_success(f"Informa√ß√£o armazenada no MongoDB: {user_input}")

    except Exception as e:
        log_error(f"Erro ao salvar no MongoDB: {str(e)}")

def load_prompt_from_file(file_path="polaris_prompt.txt"):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()
    except FileNotFoundError:
        log_warning(f"Arquivo {file_path} n√£o encontrado! Usando um prompt padr√£o.")
        return """\
        ### Instru√ß√µes:
        Voc√™ √© Polaris, um assistente inteligente.
        Responda de forma clara e objetiva, utilizando informa√ß√µes do hist√≥rico e mem√≥rias dispon√≠veis.
        Se n√£o souber a resposta, seja honesto e n√£o invente informa√ß√µes.

        Agora, aqui est√° a conversa atual:
        """

def load_keywords_from_file(file_path="keywords.txt"):
    """Carrega a lista de palavras-chave do arquivo especificado."""
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            keywords = [line.strip().lower() for line in file.readlines() if line.strip()]
            log_info(f"üìÇ Palavras-chave carregadas do arquivo ({len(keywords)} palavras).")
            return keywords
    except FileNotFoundError:
        log_warning(f"Arquivo {file_path} n√£o encontrado! Usando palavras-chave padr√£o.")
        return ["meu nome √©", "eu moro em", "eu gosto de"]

def trim_langchain_memory():
    try:
        history = memory.load_memory_variables({})["history"]

        if not isinstance(history, list):
            return

        if len(history) > LANGCHAIN_HISTORY:
            log_warning("Mem√≥ria tempor√°ria cheia, removendo mensagens mais antigas...")
            memory.chat_memory.messages = history[-LANGCHAIN_HISTORY:]

        log_success("Mem√≥ria tempor√°ria ajustada sem perda de formato!")

    except Exception as e:
        log_error(f"Erro ao ajustar mem√≥ria tempor√°ria do LangChain: {str(e)}")

from langchain.schema import HumanMessage, AIMessage

@app.post("/inference/")
async def inference(request: InferenceRequest):
    session_id = request.session_id or "default_session"
    log_info(f"üì• Nova solicita√ß√£o de infer√™ncia: {request.prompt}")

    keywords = load_keywords_from_file()

    if any(kw in request.prompt.lower() for kw in keywords):
        save_to_mongo(request.prompt)

    trim_langchain_memory()

    mongo_memories = get_memories()
    recent_memories = get_recent_memories()

    context_pieces = []
    if mongo_memories:
        context_pieces.append("üìå Mem√≥ria do Usu√°rio:\n" + "\n".join(mongo_memories))
    if recent_memories:
        context_pieces.append("üìå Conversa recente:\n" + recent_memories)

    context = "\n\n".join(context_pieces)
    prompt_instrucoes = load_prompt_from_file()
    full_prompt = f"""{prompt_instrucoes}

--- CONTEXTO ---
{context}

--- CONVERSA ATUAL ---
Usu√°rio: {request.prompt}

Polaris:"""

    resposta = llm.invoke(full_prompt)
    memory.save_context({"input": request.prompt}, {"output": resposta})

    return {"resposta": resposta}
    

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

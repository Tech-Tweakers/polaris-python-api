from llama_cpp import Llama

# Defina o caminho correto do modelo
model_path = "../../models/Meta-Llama-3-8B-Instruct.Q2_K.gguf"

print(f"ğŸ”„ Carregando modelo de: {model_path}...")
try:
    llm = Llama(model_path=model_path, verbose=True, n_ctx=8192)
    print("âœ… Modelo carregado com sucesso!")
except Exception as e:
    print(f"âŒ Erro ao carregar o modelo: {e}")
    exit(1)

# Teste simples para ver se o modelo estÃ¡ respondendo
prompt = "Qual Ã© a capital do Brasil?"
print(f"\nğŸ”¹ Enviando prompt: {prompt}")

try:
    response = llm(prompt, max_tokens=256, temperature=0.7, top_p=0.9, top_k=50)
    print("\nğŸ”¹ Resposta gerada:")
    print(response)
except Exception as e:
    print(f"âŒ Erro durante a inferÃªncia: {e}")
